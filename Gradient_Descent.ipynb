{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f9eb45a1d169>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f9eb45a1d169>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Gradient Descent\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Gradient Descent\n",
    "\n",
    "WHJ Wood July 2019\n",
    "\n",
    "Here i demonstrate the use of gradient descent for linear regression.\n",
    "\n",
    "Part 1. \n",
    "Below, I have created matrices X and y. \n",
    "y is a linear combination of the columns of x as is assumed in the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted coefficients are: \n",
      "[[2.09560912]\n",
      " [1.09983984]\n",
      " [2.1136435 ]] 3.5836887456316284\n",
      "y and predicted y are: \n",
      "[[21 28 28 25 31 28]]\n",
      "[[20.73291093 28.24168308 27.96554016 24.90609479 31.05691856 27.99747319]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Cost(y1,y2):\n",
    "    d = np.array(y1)-np.array(y2)\n",
    "    m = float(np.size(y1))\n",
    "\n",
    "    J = (1/(2*m))*np.sum(d**2)\n",
    "    return J\n",
    "\n",
    "def Linear_fit(X,y):\n",
    "    \"\"\"Uses gradient descent (least-squares) to fit a linear model\"\"\"\n",
    "    \n",
    "    Theta = np.matrix(np.ones((1,X.shape[1])))\n",
    "    print(theta)\n",
    "\n",
    "x1 = np.matrix([1,2,3,4,5,6])\n",
    "x2 = np.matrix([6,9,3,6,2,5])\n",
    "x3 = np.matrix([4,5,7,3,7,3])\n",
    "X = np.matrix(np.concatenate((x1,x2,x3), axis=0)).T\n",
    "y = 5 + X*(np.matrix([2,1,2]).T)\n",
    "\n",
    "\n",
    "\n",
    "def Linear_fit(X,y,iterations,learning_rate,regularise=False):\n",
    "    \"\"\"Uses gradient descent (least-squares) to fit a linear model\"\"\"\n",
    "    \n",
    "    # first we add a column of ones for the constant\n",
    "    X = np.concatenate((np.matrix(np.ones((X.shape[0],1))),X), axis=1)\n",
    "    \n",
    "    # Then we initiate the vector representing the cofactors in the model\n",
    "    Theta = np.matrix(np.zeros((1,X.shape[1]))).T\n",
    "    \n",
    "    cost = []\n",
    "    m=X.shape[0]\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        yhat = X*Theta\n",
    "        cost.append(Cost(yhat,y))\n",
    "        Theta = Theta - (learning_rate/m)*(X.T*(yhat-y)) # derivative of the cost function\n",
    "    coefs = Theta[1:,:]\n",
    "    intercept = float(Theta[0,0])\n",
    "    return coefs, intercept,cost\n",
    "    \n",
    "\n",
    "coefs,intercept, cost  = Linear_fit(X,y,10000,0.01)\n",
    "print(\"The fitted coefficients are: \")\n",
    "print(coefs,intercept)\n",
    "\n",
    "print(\"y and predicted y are: \")\n",
    "print(y.T)\n",
    "print(np.array(coefs.T*X.T)+intercept)\n",
    "\n",
    "plt.plot(np.array(cost))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that gradient descent has provided reasonable values for coefficient\n",
    "\n",
    "close to the true values of [2,1,2].\n",
    "\n",
    "however, the constant value of 5 i added to y has not been taken into account.\n",
    "\n",
    "This is because the algorithm has overfitted the data and not penalised the use of feature over that of the constant.\n",
    "\n",
    "Part 2. Regularisation\n",
    "\n",
    "Here, we overcome the problem of overfitting by introducing regularisation into the algorithm.\n",
    "\n",
    "the feature coefficients are penalised such that only predictive features are used and an appropriate additive constant\n",
    "\n",
    "can be found.\n",
    "\n",
    "To exacerbate the overfitting problem, this time i will add more feature which have no predictive value.\n",
    "\n",
    "Increasing the number of features without increasing the amount of data often leads to overfitting as\n",
    "\n",
    "there are more degrees of freedom in the model with more features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted coefficients without regularisation are: \n",
      "[[ 2.34896098]\n",
      " [ 1.28394471]\n",
      " [ 2.79559115]\n",
      " [-0.04822455]\n",
      " [-0.23647519]\n",
      " [-0.08976768]] 0.8340822672430663\n",
      "y and predicted y are: \n",
      "[[21 28 28 25 31 28]]\n",
      "[[20.99999997 28.00000012 27.99999996 24.99999968 30.99999996 28.00000023]]\n",
      "\n",
      "\n",
      "The fitted coefficients with regularisation are: \n",
      "[[1.97678645]\n",
      " [0.94475122]\n",
      " [1.55755922]\n",
      " [0.19527447]\n",
      " [0.24428744]\n",
      " [0.01988124]] 4.828905478031812\n",
      "y and predicted y are: \n",
      "[[21 28 28 25 31 28]]\n",
      "[[20.97174279 28.23952169 27.90720668 24.51382919 30.97843267 28.16811802]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Linear_fit(X,y,iterations,learning_rate,regularise=False,Regularisation_Coeff=0.01):\n",
    "    \"\"\"Uses gradient descent (least-squares) to fit a linear model\"\"\"\n",
    "    \n",
    "    # first we add a column of ones for the constant\n",
    "    X = np.concatenate((np.matrix(np.ones((X.shape[0],1))),X), axis=1)\n",
    "    \n",
    "    # Then we initiate the vector representing the cofactors in the model\n",
    "    Theta = np.matrix(np.zeros((1,X.shape[1]))).T\n",
    "    cost = []\n",
    "\n",
    "    m=X.shape[0]\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        yhat = X*Theta\n",
    "        cost.append(Cost(yhat,y))\n",
    "        \n",
    "        if regularise==False:\n",
    "            # standard gradient descent algorithm\n",
    "            Theta = Theta - (learning_rate/m)*(X.T*(yhat-y))\n",
    "        elif regularise==True:\n",
    "            # Regularisation of features but not of constant (Theta[0,0])\n",
    "            Theta[0,0] = Theta[0,0] - (learning_rate/m)*(np.sum(yhat-y))\n",
    "            Theta[1:,:] = Theta[1:,:]*(1- ((learning_rate*Regularisation_Coeff)/m)) - (learning_rate/m)*((X.T*(yhat-y))[1:,:])\n",
    "        else:\n",
    "            print(\"regularise must equal True or False\")\n",
    "            return None\n",
    "            \n",
    "    coefs = Theta[1:,:]\n",
    "    intercept = float(Theta[0,0])\n",
    "    return coefs, intercept, cost\n",
    "\n",
    "\n",
    "x1 = np.matrix([1,2,3,4,5,6])\n",
    "x2 = np.matrix([6,9,3,6,2,5])\n",
    "x3 = np.matrix([4,5,7,3,7,3])\n",
    "\n",
    "# this time we have 3 more (non-predictive) columns\n",
    "x4 = np.matrix([11,1,1,1,5,4])\n",
    "x5 = np.matrix([0,12,13,5,9,5])\n",
    "x6 = np.matrix([6,2,2,1,15,4])\n",
    "\n",
    "\n",
    "X = np.matrix(np.concatenate((x1,x2,x3,x4,x5,x6), axis=0)).T\n",
    "y = 5 + X*(np.matrix([2,1,2,0,0,0]).T)\n",
    "\n",
    "\n",
    "coefs,intercept, cost  = Linear_fit(X,y,10000,0.01,regularise=False)\n",
    "print(\"The fitted coefficients without regularisation are: \")\n",
    "print(coefs,intercept)\n",
    "\n",
    "print(\"y and predicted y are: \")\n",
    "print(y.T)\n",
    "print(np.array(coefs.T*X.T)+intercept)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "coefs,intercept,cost  = Linear_fit(X,y,10000,0.01,regularise=True,Regularisation_Coeff=0.45)\n",
    "print(\"The fitted coefficients with regularisation are: \")\n",
    "print(coefs,intercept)\n",
    "\n",
    "print(\"y and predicted y are: \")\n",
    "print(y.T)\n",
    "print(np.array(coefs.T*X.T)+intercept)\n",
    "\n",
    "plt.plot(np.array(cost))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constan of 4.8 here is much closer to the actual value of 5.\n",
    "and so even with a higher number of features, it is possible to prevent overfitting with the use of regularisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
